{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from util.patch_embed import PatchEmbed_org\n",
    "from models_mae import MaskedAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoder(embed_dim=768, do_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(spec_batch, in_chanel, embed_dim, patch_size=16, smallest_length=1024):\n",
    "    # the default longest length of a spectrogram is 1024\n",
    "    padded_specs = torch.tensor([])\n",
    "    embeder = nn.Conv2d(in_chanel, embed_dim, kernel_size=(patch_size, patch_size), stride=(patch_size, patch_size), bias=False)\n",
    "    embeder.weight.requires_grad = False\n",
    "    N, C, H, W = spec_batch.shape\n",
    "    longest = smallest_length\n",
    "\n",
    "    # find the longest \n",
    "    for idx in range(N):\n",
    "        spec = spec_batch[idx, :, :, :]\n",
    "        spec_c, spec_h, spec_w = spec.shape\n",
    "        if spec_w > longest:\n",
    "            longest = spec_w\n",
    "\n",
    "    # pad the spectrogram\n",
    "    for idx in range(N):\n",
    "        spec = spec_batch[idx, :, :, :]\n",
    "        spec_c, spec_h, spec_w = spec.shape\n",
    "        if spec_w < longest:\n",
    "            pads = torch.zeros(spec_c, spec_h, longest - spec_w)\n",
    "            padded_spec = torch.cat([spec, pads], dim=-1).unsqueeze(0)\n",
    "            padded_specs = torch.cat([padded_specs, padded_spec], dim=0)\n",
    "\n",
    "    # get the padding mask\n",
    "    padding_masks = embeder(padded_specs).flatten(2).transpose(1, 2)\n",
    "    padding_masks = torch.where(padding_masks == 0, 1, 0).bool()\n",
    "\n",
    "    return padded_specs, padding_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 1, 128, 1001)\n",
    "a, masks = padding(a, 1, 768, 16, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 768])\n",
      "torch.Size([2, 512, 512])\n",
      "torch.Size([2, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "print(masks.shape)\n",
    "print(masks[:, :, :512].shape)\n",
    "print(masks[:, :, :256].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1.5013, 1.5960, 1.5284,  ..., 1.4298, 1.4465, 0.0000],\n",
       "         [1.5653, 1.5508, 1.5583,  ..., 1.5301, 1.4107, 0.0000]],\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([[[-0.1342, -1.7399,  2.5878,  ...,  1.4804,  1.3534,  1.0617],\n",
       "          [-0.1890, -1.7116,  2.8103,  ...,  1.5090,  1.4721,  1.0208],\n",
       "          [-0.2631, -1.5977,  3.0126,  ...,  1.5937,  1.0246,  1.2272],\n",
       "          ...,\n",
       "          [-0.1900, -2.7658,  2.5324,  ...,  0.8216,  0.5004,  1.2674],\n",
       "          [-0.0825, -2.7024,  2.5177,  ...,  0.8202,  0.5130,  1.2409],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2108, -1.7987,  2.5642,  ...,  1.4829,  1.3316,  0.8886],\n",
       "          [-0.1336, -1.7770,  2.9069,  ...,  1.5689,  1.3558,  1.1317],\n",
       "          [-0.2668, -1.5569,  3.0893,  ...,  1.3738,  1.0580,  1.2533],\n",
       "          ...,\n",
       "          [-0.1837, -2.8484,  2.5315,  ...,  0.7940,  0.5916,  1.2866],\n",
       "          [-0.0241, -2.7930,  2.5626,  ...,  0.8455,  0.5201,  1.2673],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([0.], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a, padding_mask=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(imgs):\n",
    "    h = imgs.shape[2] // 16\n",
    "    w = imgs.shape[3] // 16\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 1, h, 16, w, 16))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, 16**2 * 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = patchify(torch.rand(2, 1, 128, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiomae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
